{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm, trange\n",
    "import imageio\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def config_parser():\n",
    "\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\"--expname\", type=str,\n",
    "                        help='experiment name')\n",
    "    parser.add_argument(\"--basedir\", type=str, default='./logs/',\n",
    "                        help='where to store ckpts and logs')\n",
    "    parser.add_argument(\"--datadir\", type=str, default='./data/llff/fern',\n",
    "                        help='input data directory')\n",
    "\n",
    "    # training options\n",
    "    parser.add_argument(\"--netdepth\", type=int, default=8,\n",
    "                        help='layers in network')\n",
    "    parser.add_argument(\"--netwidth\", type=int, default=256,\n",
    "                        help='channels per layer')\n",
    "    parser.add_argument(\"--netdepth_fine\", type=int, default=8,\n",
    "                        help='layers in fine network')\n",
    "    parser.add_argument(\"--netwidth_fine\", type=int, default=256,\n",
    "                        help='channels per layer in fine network')\n",
    "    parser.add_argument(\"--N_rand\", type=int, default=32*32*4,\n",
    "                        help='batch size (number of random rays per gradient step)')\n",
    "    parser.add_argument(\"--lrate\", type=float, default=5e-4,\n",
    "                        help='learning rate')\n",
    "    parser.add_argument(\"--lrate_decay\", type=int, default=250,\n",
    "                        help='exponential learning rate decay (in 1000 steps)')\n",
    "    parser.add_argument(\"--chunk\", type=int, default=1024*32,\n",
    "                        help='number of rays processed in parallel, decrease if running out of memory')\n",
    "    parser.add_argument(\"--netchunk\", type=int, default=1024*64,\n",
    "                        help='number of pts sent through network in parallel, decrease if running out of memory')\n",
    "    parser.add_argument(\"--no_batching\", action='store_true',\n",
    "                        help='only take random rays from 1 image at a time')\n",
    "    parser.add_argument(\"--no_reload\", action='store_true',\n",
    "                        help='do not reload weights from saved ckpt')\n",
    "    parser.add_argument(\"--ft_path\", type=str, default=None,\n",
    "                        help='specific weights npy file to reload for coarse network')\n",
    "\n",
    "    # rendering options\n",
    "    parser.add_argument(\"--N_samples\", type=int, default=64,\n",
    "                        help='number of coarse samples per ray')\n",
    "    parser.add_argument(\"--N_importance\", type=int, default=0,\n",
    "                        help='number of additional fine samples per ray')\n",
    "    parser.add_argument(\"--perturb\", type=float, default=1.,\n",
    "                        help='set to 0. for no jitter, 1. for jitter')\n",
    "    parser.add_argument(\"--use_viewdirs\", action='store_true',\n",
    "                        help='use full 5D input instead of 3D')\n",
    "    parser.add_argument(\"--i_embed\", type=int, default=0,\n",
    "                        help='set 0 for default positional encoding, -1 for none')\n",
    "    parser.add_argument(\"--multires\", type=int, default=10,\n",
    "                        help='log2 of max freq for positional encoding (3D location)')\n",
    "    parser.add_argument(\"--multires_views\", type=int, default=4,\n",
    "                        help='log2 of max freq for positional encoding (2D direction)')\n",
    "    parser.add_argument(\"--raw_noise_std\", type=float, default=0.,\n",
    "                        help='std dev of noise added to regularize sigma_a output, 1e0 recommended')\n",
    "\n",
    "    parser.add_argument(\"--render_only\", action='store_true',\n",
    "                        help='do not optimize, reload weights and render out render_poses path')\n",
    "    parser.add_argument(\"--render_test\", action='store_true',\n",
    "                        help='render the test set instead of render_poses path')\n",
    "    parser.add_argument(\"--render_factor\", type=int, default=0,\n",
    "                        help='downsampling factor to speed up rendering, set 4 or 8 for fast preview')\n",
    "\n",
    "    # training options\n",
    "    parser.add_argument(\"--precrop_iters\", type=int, default=0,\n",
    "                        help='number of steps to train on central crops')\n",
    "    parser.add_argument(\"--precrop_frac\", type=float,\n",
    "                        default=.5, help='fraction of img taken for central crops')\n",
    "\n",
    "    # dataset options\n",
    "    parser.add_argument(\"--dataset_type\", type=str, default='llff',\n",
    "                        help='options: llff / blender / deepvoxels')\n",
    "    parser.add_argument(\"--testskip\", type=int, default=8,\n",
    "                        help='will load 1/N images from test/val sets, useful for large datasets like deepvoxels')\n",
    "\n",
    "    ## deepvoxels flags\n",
    "    parser.add_argument(\"--shape\", type=str, default='greek',\n",
    "                        help='options : armchair / cube / greek / vase')\n",
    "\n",
    "    ## blender flags\n",
    "    parser.add_argument(\"--white_bkgd\", action='store_true',\n",
    "                        help='set to render synthetic data on a white bkgd (always use for dvoxels)')\n",
    "    parser.add_argument(\"--half_res\", action='store_true',\n",
    "                        help='load blender synthetic data at 400x400 instead of 800x800')\n",
    "\n",
    "    ## llff flags\n",
    "    parser.add_argument(\"--factor\", type=int, default=8,\n",
    "                        help='downsample factor for LLFF images')\n",
    "    parser.add_argument(\"--no_ndc\", action='store_true',\n",
    "                        help='do not use normalized device coordinates (set for non-forward facing scenes)')\n",
    "    parser.add_argument(\"--lindisp\", action='store_true',\n",
    "                        help='sampling linearly in disparity rather than depth')\n",
    "    parser.add_argument(\"--spherify\", action='store_true',\n",
    "                        help='set for spherical 360 scenes')\n",
    "    parser.add_argument(\"--llffhold\", type=int, default=8,\n",
    "                        help='will take every 1/N images as LLFF test set, paper uses 8')\n",
    "\n",
    "    # logging/saving options\n",
    "    parser.add_argument(\"--i_print\",   type=int, default=100,\n",
    "                        help='frequency of console printout and metric loggin')\n",
    "    parser.add_argument(\"--i_img\",     type=int, default=500,\n",
    "                        help='frequency of tensorboard image logging')\n",
    "    parser.add_argument(\"--i_weights\", type=int, default=10000,\n",
    "                        help='frequency of weight ckpt saving')\n",
    "    parser.add_argument(\"--i_testset\", type=int, default=50000,\n",
    "                        help='frequency of testset saving')\n",
    "    parser.add_argument(\"--i_video\",   type=int, default=50000,\n",
    "                        help='frequency of render_poses video saving')\n",
    "\n",
    "    return parser"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "parser = config_parser()\n",
    "args = parser.parse_args(args=[])\n",
    "args.config = 'configs/lego.txt'\n",
    "\n",
    "# load config\n",
    "\n",
    "with open(args.config, 'r') as fp:\n",
    "    lines = fp.readlines()\n",
    "    lines = [line.strip() for line in lines]\n",
    "    lines = [line.split(' = ') for line in lines if len(line)>0]\n",
    "\n",
    "configs = {}\n",
    "for line in lines:\n",
    "    configs[line[0]] = line[1]\n",
    "\n",
    "print(configs)\n",
    "\n",
    "args.expname = configs['expname']\n",
    "args.basedir = configs['basedir']\n",
    "args.datadir = configs['datadir']\n",
    "args.dataset_type = configs['dataset_type']\n",
    "args.no_batching = configs['no_batching']\n",
    "args.use_viewdirs = configs['use_viewdirs']\n",
    "args.white_bkgd = configs['white_bkgd']\n",
    "args.lrate_decay = int(configs['lrate_decay'])\n",
    "args.N_samples = int(configs['N_samples'])\n",
    "args.N_importance = int(configs['N_importance'])\n",
    "args.N_rand = int(configs['N_rand'])\n",
    "args.precrop_iters = int(configs['precrop_iters'])\n",
    "args.precrop_frac = float(configs['precrop_frac'])\n",
    "args.half_res = configs['half_res']\n",
    "\n",
    "args.no_reload = True\n",
    "print(args)\n",
    "\n",
    "\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('mps') if torch.backends.mps.is_available() else 'cpu'\n",
    "device = 'cpu'\n",
    "DEBUG=False\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_datadir_lego = './data/nerf_synthetic/lego'\n",
    "with open(os.path.join(_datadir_lego,'transforms_test.json'), 'r') as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "_frames = meta['frames']\n",
    "\n",
    "fig, ax = plt.subplots(1,5, figsize=(30,300))\n",
    "ax = ax.flatten()\n",
    "for i in range(5):\n",
    "    _frame = _frames[i]\n",
    "    _filename = os.path.join(_datadir_lego, _frame['file_path']+'.png')\n",
    "    img = plt.imread(_filename)\n",
    "    ax[i].imshow(img)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_datadir_llff = './data/nerf_llff_data/fern'\n",
    "_filenames = [os.path.join(_datadir_llff,'images',f) for f in sorted(os.listdir(os.path.join(_datadir_llff, 'images'))) if (f.endswith('.JPG') or f.endswith('.jpg') or f.endswith('.png'))]\n",
    "\n",
    "fig, ax = plt.subplots(1,5, figsize=(30,600))\n",
    "for i in range(5):\n",
    "    _filename = _filenames[i]\n",
    "    img = plt.imread(_filename)\n",
    "    ax[i].imshow(img)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trans_t = lambda t : torch.Tensor([  #transformation 행렬 Notion 참고 (z축에 대한 거리)\n",
    "    [1,0,0,0],\n",
    "    [0,1,0,0],\n",
    "    [0,0,1,t],\n",
    "    [0,0,0,1]]).float()\n",
    "\n",
    "rot_phi = lambda phi : torch.Tensor([\n",
    "    [1,0,0,0],\n",
    "    [0,np.cos(phi),-np.sin(phi),0],\n",
    "    [0,np.sin(phi),np.cos(phi),0],\n",
    "    [0,0,0,1]]).float()\n",
    "\n",
    "rot_theta = lambda theta : torch.Tensor([\n",
    "    [np.cos(theta),0,np.sin(theta),0],\n",
    "    [0,1,0,0],\n",
    "    [np.sin(theta),0,np.cos(theta),0],\n",
    "    [0,0,0,1]]).float()\n",
    "\n",
    "def pose_spherical(theta, phi, radius): #render시 pose 를 조정하기 위\n",
    "    c2w = trans_t(radius)\n",
    "    c2w = rot_phi(phi/180*np.pi) @ c2w\n",
    "    c2w = rot_theta(theta/180*np.pi) @ c2w\n",
    "    c2w = torch.Tensor(np.array([[-1,0,0,0],\n",
    "                                 [0,0,1,0],\n",
    "                                 [0,1,0,0],\n",
    "                                 [0,0,0,1]])) @ c2w #world좌표의 y,z를 서로 바꾸고, x축의 부호를 바꾼다.\n",
    "    return c2w"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_rays(H,W,K,c2w):\n",
    "    i, j = torch.meshgrid(torch.linspace(0,W-1,W), torch.linspace(0,H-1,H))\n",
    "    i = i.t() #numpy의 축방향과 이미지의 축방향이 다르므로\n",
    "    j = j.t()\n",
    "    dirs = torch.stack([(i-K[0][2])/K[0][0],-(j-K[1][2])/K[1][1], torch.ones_like(i)],-1) #이미지에서 y축 증가방향은 아래로 내려가는 방향이므로 음수, z dir는 -1이라고 설정\n",
    "    rays_d = torch.sum(dirs[..., np.newaxis, :] * c2w[:3,:3], -1)\n",
    "    rays_o = c2w[:3,-1].expand(rays_d.shape)\n",
    "    return rays_o, rays_d\n",
    "\n",
    "def get_rays_np(H, W, K, c2w):\n",
    "    i, j = np.meshgrid(np.arange(W, dtype=np.float32), np.arange(H, dtype=np.float32), indexing='xy')\n",
    "    dirs = np.stack([(i-K[0][2])/K[0][0], -(j-K[1][2])/K[1][1], -np.ones_like(i)], -1)\n",
    "    # Rotate ray directions from camera frame to the world frame\n",
    "    rays_d = np.sum(dirs[..., np.newaxis, :] * c2w[:3,:3], -1)  # dot product, equals to: [c2w.dot(dir) for dir in dirs]\n",
    "    # Translate camera frame's origin to the world frame. It is the origin of all rays.\n",
    "    rays_o = np.broadcast_to(c2w[:3,-1], np.shape(rays_d))\n",
    "    return rays_o, rays_d"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#dataloader\n",
    "\n",
    "def load_blender_data(basedir, half_res=False, testskip=1):\n",
    "    #img & poses\n",
    "    splits = ['train', 'val', 'test']\n",
    "    metas = {}\n",
    "    for s in splits:\n",
    "        with open(os.path.join(basedir, 'transforms_{}.json'.format(s)), 'r') as f:\n",
    "            metas[s] = json.load(f)\n",
    "\n",
    "    all_imgs = []\n",
    "    all_poses = []\n",
    "    counts = [0]\n",
    "    for s in splits:\n",
    "        meta = metas[s]\n",
    "        imgs = []\n",
    "        poses = []\n",
    "        if s == 'train' or testskip==0:\n",
    "            skip = 1\n",
    "        else:\n",
    "            skip = testskip\n",
    "\n",
    "        for frame in meta['frames'][::skip]:\n",
    "            filename = os.path.join(basedir, frame['file_path']+'.png')\n",
    "            imgs.append(plt.imread(filename))\n",
    "            poses.append(np.array(frame['transform_matrix']))\n",
    "\n",
    "        imgs = np.array(imgs).astype(np.float32)\n",
    "        poses = np.array(poses).astype(np.float32)\n",
    "        counts.append(counts[-1] + imgs.shape[0])\n",
    "        all_imgs.append(imgs)\n",
    "        all_poses.append(poses)\n",
    "\n",
    "    i_split = [np.arange(counts[i], counts[i+1]) for i in range(3)]\n",
    "    imgs = np.concatenate(all_imgs, 0) #?\n",
    "    poses = np.concatenate(all_poses, 0)\n",
    "\n",
    "    #camera intrinsics\n",
    "    H, W = imgs[0].shape[:2]\n",
    "    camera_angle_x = float(meta['camera_angle_x'])\n",
    "    focal = .5 * W / np.tan(.5 * camera_angle_x)\n",
    "\n",
    "    render_poses = torch.stack([pose_spherical(angle, -30.0, 4.0) for angle in np.linspace(-180, 180, 40+1)[:-1]],0)\n",
    "    ##??\n",
    "\n",
    "    if half_res:\n",
    "        H = H//2\n",
    "        W = W//2\n",
    "        focal = focal/2\n",
    "\n",
    "        imgs_half_res = np.zeros((imgs.shape[0],H, W, 4))\n",
    "        for i, img in enumerate(imgs):\n",
    "            imgs_half_res[i] = cv2.resize(img, (W,H), interpolation=cv2.INTER_AREA)\n",
    "        imgs = imgs_half_res\n",
    "\n",
    "    return imgs, poses, render_poses, [H,W,focal], i_split\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#positional encoding\n",
    "\n",
    "class Embedder:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.kwargs = kwargs\n",
    "        self.create_embedding_fn()\n",
    "\n",
    "    def create_embedding_fn(self):\n",
    "        embed_fns = []\n",
    "        d = self.kwargs['input_dims']\n",
    "        out_dim = 0\n",
    "        if self.kwargs['include_input']:\n",
    "            embed_fns.append(lambda x : x)\n",
    "            out_dim += d\n",
    "\n",
    "        max_freq = self.kwargs['max_freq_log2'] #L-1\n",
    "        N_freqs = self.kwargs['num_freqs'] #L\n",
    "\n",
    "        if self.kwargs['log_sampling']:\n",
    "            freq_bands = 2.**torch.linspace(0., max_freq, N_freqs)\n",
    "        else:\n",
    "            freq_bands = torch.linspace(2.**0, 2.**max_freq, N_freqs)\n",
    "\n",
    "        for freq in freq_bands:\n",
    "            for p_fn in self.kwargs['periodic_fns']:\n",
    "                embed_fns.append(lambda x, p_fn=p_fn, freq=freq : p_fn(x * freq))\n",
    "                out_dim += d\n",
    "\n",
    "        self.embed_fns = embed_fns\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "    def embed(self, inputs):\n",
    "        return torch.cat([fn(inputs) for fn in self.embed_fns], -1)\n",
    "\n",
    "def get_embedder(multires, i=0):\n",
    "    if i == -1:\n",
    "        return nn.identity(), 3\n",
    "\n",
    "    embed_kwargs = {\n",
    "        'include_input' : True,\n",
    "        'input_dims' : 3,\n",
    "        'max_freq_log2' : multires - 1,\n",
    "        'num_freqs' : multires,\n",
    "        'log_sampling' : True,\n",
    "        'periodic_fns' : [torch.sin, torch.cos],\n",
    "    }\n",
    "\n",
    "    embedder_obj = Embedder(**embed_kwargs)\n",
    "    embed = lambda x, eo=embedder_obj : eo.embed(x)\n",
    "    return embed, embedder_obj.out_dim"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class NeRF(nn.Module):\n",
    "    def __init__(self, D=8, W=256, input_ch=3, input_ch_views=3, output_ch=4, skips=[4,], use_viewdirs=False ):\n",
    "        super(NeRF, self).__init__()\n",
    "        self.D = D\n",
    "        self.W = W\n",
    "        self.input_ch = input_ch\n",
    "        self.input_ch_view = input_ch_views\n",
    "        self.output_ch = output_ch\n",
    "        self.skips = skips\n",
    "        self.use_viewdirs = use_viewdirs\n",
    "\n",
    "        self.pts_linears = nn.ModuleList([nn.Linear(input_ch, W)] + [nn.Linear(W, W) if i not in self.skips else nn.Linear(W+input_ch, W) for i in range(D-1)])\n",
    "        self.view_linears = nn.ModuleList([nn.Linear(input_ch_views + W, W//2)])\n",
    "\n",
    "        if use_viewdirs:\n",
    "            self.feature_linear = nn.Linear(W, W)\n",
    "            self.alpha_linear = nn.Linear(W, 1)\n",
    "            self.rgb_linear = nn.Linear(W//2, 3)\n",
    "        else:\n",
    "            self.output_linear = nn.Linear(W, output_ch)\n",
    "\n",
    "    def forward(self, x): #input은 x,d가 concat 되어 들어옴\n",
    "        input_pts, input_views = torch.split(x, [self.input_ch, self.input_ch_view], dim=-1)\n",
    "        h = input_pts\n",
    "        for i, l in enumerate(self.pts_linears):\n",
    "            h = self.pts_linears[i](h)\n",
    "            h = F.relu(h)\n",
    "            if i in self.skips:\n",
    "                h = torch.concat([input_pts, h], -1) #(B, 63+256)\n",
    "\n",
    "        if self.use_viewdirs:\n",
    "            alpha = self.alpha_linear(h) #density\n",
    "            feature = self.feature_linear(h)\n",
    "            h = torch.cat([feature, input_views], -1)\n",
    "\n",
    "            for i, l in enumerate(self.view_linears):\n",
    "                h = self.view_linears[i](h)\n",
    "                h = F.relu(h)\n",
    "\n",
    "            rgb = self.rgb_linear(h)\n",
    "            outputs = torch.cat([rgb, alpha], -1)\n",
    "        else:\n",
    "            outputs = self.output_linear(h)\n",
    "\n",
    "\n",
    "        return outputs\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##test\n",
    "input = torch.rand(1,3)\n",
    "input_dir = torch.rand(1,3)\n",
    "\n",
    "embed, out_dim = get_embedder(10)\n",
    "embed_dir, out_dim_dir = get_embedder(4)\n",
    "emb_input = embed(input)\n",
    "emb_input_dir = embed_dir(input_dir)\n",
    "\n",
    "model = NeRF(input_ch=out_dim, input_ch_views=out_dim_dir, use_viewdirs=True)\n",
    "\n",
    "x = torch.cat([emb_input, emb_input_dir], dim=-1)\n",
    "print(x.shape)\n",
    "out = model(x)\n",
    "\n",
    "print(out)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def batchify(fn, chunk): #NeRF모델에 input을 배치단위로 넣어주는 함수\n",
    "    if chunk is None:\n",
    "        return fn\n",
    "\n",
    "    def ret(inputs):\n",
    "        return torch.cat([fn(inputs[i:i+chunk]) for i in range(0, inputs.shape[0], chunk)], 0)\n",
    "\n",
    "    return fn\n",
    "\n",
    "def run_network(inputs, viewdirs, fn, embed_fn, embeddirs_fn, netchunk=1024*64):\n",
    "    #input : N_Rays(H*W), N_samples, 3(xyz) -> 앞의 두개의 dimension을 flatten\n",
    "    inputs_flat = torch.reshape(inputs, [-1, inputs.shape[-1]])\n",
    "    embedded = embed_fn(inputs_flat)\n",
    "\n",
    "    if viewdirs is not None:\n",
    "        #하나의 ray에서 viewdir는 모두 같다. -> ray의 수만큼만 존재 input(N_rays, 3)\n",
    "        input_dirs = viewdirs[:, None].expand(inputs.shape) #(N_rays, 1, 3) -> (N_rays, N_samples, 3)\n",
    "        input_dirs_flat = torch.reshape(input_dirs, [-1, input_dirs.shape[-1]])\n",
    "        embedded_dirs = embeddirs_fn(input_dirs_flat)\n",
    "        embedded = torch.cat([embedded, embedded_dirs], -1)\n",
    "\n",
    "    outputs_flat = batchify(fn, netchunk)(embedded)\n",
    "    outputs = torch.reshape(outputs_flat, list(inputs.shape[:-1]) + [outputs_flat.shape[-1]])\n",
    "    return outputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_nerf(args):\n",
    "    embed_fn, input_ch = get_embedder(args.multires, args.i_embed)\n",
    "    input_ch_views = 0\n",
    "    embeddirs_fn = None\n",
    "    if args.use_viewdirs:\n",
    "        embeddirs_fn, input_ch_views = get_embedder(args.multires_views, args.i_embed)\n",
    "    output_ch = 5 if args.N_importance > 0 else 4 #Coarse Network에서 importance(color앞에 곱해지는 값들)를 추출하고 importance sampling된 곳들을 model fine에 넣는다. (dim5)\n",
    "    skips = [4]\n",
    "\n",
    "    model = NeRF(D=args.netdepth, W=args.netwidth, input_ch=input_ch, output_ch=output_ch, skips=skips, input_ch_views=input_ch_views, use_viewdirs=args.use_viewdirs).to(device)\n",
    "\n",
    "    grad_vars = list(model.parameters())\n",
    "\n",
    "    if args.N_importance > 0:\n",
    "        model_fine = NeRF(D=args.netdepth_fine, W=args.netwidth_fine, input_ch=input_ch, output_ch=output_ch, skips=skips, input_ch_views=input_ch_views, use_viewdirs=args.use_viewdirs).to(device)\n",
    "\n",
    "        grad_vars += list(model_fine.parameters())\n",
    "\n",
    "    network_query_fn = lambda inputs, viewdirs, network_fn : run_network(inputs, viewdirs, network_fn, embed_fn=embed_fn, embeddirs_fn=embeddirs_fn, netchunk=args.netchunk) #input, viewdirs, network_fn 말고 embedding 부분은 계속 같은 값들을 사용하므로 이를 다시 쓰지 않기 위한 편의함수\n",
    "\n",
    "    optimizer = torch.optim.Adam(params=grad_vars, lr=args.lrate, betas=(0.9, 0.999))\n",
    "    start = 0\n",
    "    basedir = args.basedir\n",
    "    expname = args.expname\n",
    "\n",
    "    ######load ckpt\n",
    "    if args.ft_path is not None and args.ft_path!='None':\n",
    "        ckpts = [args.ft_path]\n",
    "    else:\n",
    "        ckpts = [os.path.join(basedir, expname, f) for f in sorted(os.listdir(os.path.join(basedir, expname))) if 'tar' in f]\n",
    "\n",
    "    print('found ckpts', ckpts)\n",
    "    if len(ckpts) > 0 and not args.no_reload:\n",
    "        ckpt_path = ckpts[-1]\n",
    "        print('Reloading from', ckpt_path)\n",
    "        ckpt = torch.load(ckpt_path)\n",
    "\n",
    "        start = ckpt['global step']\n",
    "        optimizer.load_state_dict(ckpt['optimizer_state.dict'])\n",
    "\n",
    "        model.load_state_dict(ckpt['network_fn_state_dict'])\n",
    "        if model_fine is not None:\n",
    "            model_fine.load_state_dict(ckpt['network_fine_state_dict'])\n",
    "\n",
    "    ##########\n",
    "\n",
    "    render_kwargs_train = {\n",
    "        'network_query_fn' : network_query_fn,\n",
    "        'perturb' : args.perturb,\n",
    "        'N_importance' : args.N_importance,\n",
    "        'network_fine' : model_fine,\n",
    "        'N_samples' : args.N_samples,\n",
    "        'network_fn' : model,\n",
    "        'use_viewdirs' : args.use_viewdirs,\n",
    "        'white_bkgd' : args.white_bkgd,\n",
    "        'raw_noise_std' : args.raw_noise_std,\n",
    "    }\n",
    "\n",
    "    if args.dataset_type != 'llff' or args.no_ndc:\n",
    "        print('Not ndc!')\n",
    "        render_kwargs_train['ndc'] = False\n",
    "        render_kwargs_train['lindisp'] = args.lindisp\n",
    "\n",
    "    render_kwargs_test = {k: render_kwargs_train[k] for k in render_kwargs_train}\n",
    "    render_kwargs_test['perturb'] = False\n",
    "    render_kwargs_test['raw_noise_std'] = 0\n",
    "\n",
    "    return render_kwargs_train, render_kwargs_test, start, grad_vars, optimizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sample_pdf(bins, weights, N_samples, det=False, pytest=False):\n",
    "    #pdf를 구한후 cdf를 구하면 0~1사이의 random한 값을 inverse cdf를 통해 구하면 확률밀도가 높은 지역값이 많이 나오는 효과가 생긴다.\n",
    "    weights = weights + 1e-5\n",
    "    pdf = weights / torch.sum(weights, -1, keepdim=True) #확률로 변환\n",
    "    cdf = torch.cumsum(pdf, -1) #batch x len(bins) -1\n",
    "    cdf = torch.cat([torch.zeros_like(cdf[...,:1]), cdf], -1) #batch x len(bins)\n",
    "\n",
    "    if det:\n",
    "        u = torch.linspace(0., 1., steps=N_samples)\n",
    "        u = u.expand(list(cdf.shape[:-1])+[N_samples]) #batch x N_samples\n",
    "    else:\n",
    "        u = torch.rand(list(cdf.shape[:-1])+[N_samples])\n",
    "\n",
    "    if pytest: #test를 위해 고정된 random값 생성하는 것 -> 중요 X\n",
    "        np.random.seed(0)\n",
    "        new_shape = list(cdf.shape[:-1])+[N_samples]\n",
    "        if det:\n",
    "            u = np.linspace(0., 1., steps=N_samples)\n",
    "            u = u.broadcast_to(u, new_shape)\n",
    "        else:\n",
    "            u = np.random.rand(*new_shape)\n",
    "        u = torch.Tensor(u)\n",
    "\n",
    "    u = u.contiguous()\n",
    "    inds = torch.searchsorted(cdf, u, right=True) #inverse구하는 느낌의 함수로 query 값(u)이 주어진 array(cdf)의 몇번째 index로 들어갈 수 있는지를 반환한다.\n",
    "    below = torch.max(torch.zeros_like(inds-1), inds-1) #inds만으로도 할 수 있는데 조금 더 엄밀한 방법\n",
    "    above = torch.min((cdf.shape[-1]-1)*torch.ones_like(inds), inds)\n",
    "    inds_g = torch.stack([below, above], -1) #batch x N_samples, 2\n",
    "\n",
    "    matched_shape = [inds_g.shape[0], inds_g.shape[1], cdf.shape[-1]]\n",
    "    cdf_g = torch.gather(cdf.unsqueeze(1).expand(matched_shape), 2, inds_g)\n",
    "    bins_g = torch.gather(bins.unsqueeze(1).expand(matched_shape), 2, inds_g)\n",
    "\n",
    "    denom = (cdf_g[...,1]-cdf_g[...,0])\n",
    "    denom = torch.where(denom < 1e-5, torch.ones_like(denom), denom)\n",
    "    t = (u-cdf_g[...,0])/denom\n",
    "    samples = bins_g[...,0] + t*(bins_g[...,1]-bins_g[...,0])\n",
    "\n",
    "    return samples"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def raw2outputs(raw, z_vals, rays_d, raw_noise_std=0, white_bkgd=False, pytest=False):\n",
    "    \"\"\"\n",
    "    raw : N_rays x N_samples x 4\n",
    "    z_vals : N_rays x N_samples\n",
    "    rays_d : N_rays x 3\n",
    "    :return: rgb_map, disp_map, acc_map, weights, depth_map\n",
    "    \"\"\"\n",
    "\n",
    "    raw2alpha = lambda raw, dists, act_fn=F.relu : 1. -torch.exp(-act_fn(raw)*dists) #1-exp(-sigma*delta)\n",
    "\n",
    "    dists = z_vals[..., 1:] - z_vals[...,:-1]\n",
    "    dists = torch.cat([dists, torch.Tensor([1e10]).expand(dists[...,:1].shape)], -1) #N_rays, N_samples\n",
    "    dists = dists * torch.norm(rays_d[..., None, :], dim=-1) #방향을 고려\n",
    "\n",
    "    rgb = torch.sigmoid(raw[..., :3])\n",
    "    noise = 0.\n",
    "    if raw_noise_std > 0:\n",
    "        noise = torch.randn(raw[...,:3].shape) * raw_noise_std #overfitting 방지\n",
    "\n",
    "    alpha = raw2alpha(raw[...,3] + noise, dists)\n",
    "    weights = alpha * torch.cumprod(torch.cat([torch.ones((alpha.shape[0], 1)), 1.-alpha + 1e-10], -1), -1)[:,:-1]  #T*alpha #N_rays x N_samples\n",
    "\n",
    "    rgb_map = torch.sum(weights[..., None] * rgb, -2)\n",
    "    depth_map = torch.sum(weights * z_vals, -1)\n",
    "    disp_map = 1./torch.max(1e-10 * torch.ones_like(depth_map), depth_map/torch.sum(weights, -1))\n",
    "    acc_map = torch.sum(weights, -1) #occupancy map\n",
    "\n",
    "    return rgb_map, disp_map, acc_map, weights, depth_map"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def render_rays(ray_batch, network_fn, network_query_fn, N_samples, retraw=False, lindisp=False, perturb=0, N_importance=0, network_fine=None, white_bkgd=False, raw_noise_std=0, verbose=False, pytest=False):\n",
    "    #ray batch : ray위에서 샘플링하기 위해 필요한 정보들, origin, direction, min/max dist 등..\n",
    "    N_rays = ray_batch.shape[0]\n",
    "    rays_o, rays_d = ray_batch[:, 0:3], ray_batch[:, 3:6]\n",
    "    viewdirs = ray_batch[:, -3:] if ray_batch.shape[-1] > 8 else None\n",
    "    bounds = torch.reshape(ray_batch[...,6:8], [-1,1,2])\n",
    "    near, far = bounds[..., 0], bounds[..., 1]\n",
    "\n",
    "    t_vals = torch.linspace(0., 1., steps=N_samples)\n",
    "    if not lindisp:\n",
    "        z_vals = near * (1.-t_vals) + far*(t_vals) #linear한 t를 통해 near과 far사이의 z를 샘플링\n",
    "    else:\n",
    "        z_vals = 1./(1./near * (1/-t_vals) + 1./far * t_vals)\n",
    "\n",
    "    z_vals = z_vals.expand([N_rays, N_samples])\n",
    "\n",
    "    if perturb > 0:\n",
    "        #동일한 간격이 아닌 구간내에서 random sampling\n",
    "        mids = .5 * (z_vals[...,1:]+z_vals[...,:-1]) #샘플들의 중점 (N_rays x N_samples-1)\n",
    "        upper = torch.cat([mids, z_vals[..., -1:]], -1) #(N_rays x N_samples)\n",
    "        lower = torch.cat([z_vals[...,:1], mids], -1)\n",
    "        t_rand = torch.rand(z_vals.shape)\n",
    "\n",
    "        if pytest:\n",
    "            np.random.seed(0)\n",
    "            t_rand = np.random.rand(*list(z_vals.shape))\n",
    "            t_rand = torch.Tensor(t_rand)\n",
    "\n",
    "        z_vals = lower + (upper - lower) * t_rand\n",
    "\n",
    "    pts = rays_o[...,None,:] + rays_d[...,None,:] * z_vals[...,:,None] #ray = o+td\n",
    "\n",
    "    raw = network_query_fn(pts, viewdirs, network_fn) #Coarse의 c (N_ray, N_samples, 4)\n",
    "    rgb_map, disp_map, acc_map, weights, depth_map = raw2outputs(raw, z_vals, rays_d, raw_noise_std, white_bkgd, pytest=pytest)\n",
    "\n",
    "    if N_importance > 0: #fine run_network (중요한 점의 개수)\n",
    "        rgb_map_0, disp_map_0, acc_map_0 = rgb_map, disp_map, acc_map\n",
    "\n",
    "        z_vals_mid = .5 * (z_vals[...,1:] + z_vals[...,:-1])\n",
    "        z_samples = sample_pdf(z_vals_mid, weights[...,1:-1], N_importance, det=(perturb==0), pytest=pytest) #weight가 높은 곳에서 새로운 sample위치들을 얻는 과정\n",
    "        z_samples = z_samples.detach()\n",
    "\n",
    "        z_vals, _ = torch.sort(torch.cat([z_vals, z_samples], -1), -1) #N_rays, N_samples + N_importance\n",
    "        pts = rays_o[...,None,:] + rays_d[...,None,:] * z_vals[...,:,None] #ray = o+td\n",
    "\n",
    "        run_fn = network_fn if network_fine is None else network_fine\n",
    "        raw = network_query_fn(pts, viewdirs, run_fn)\n",
    "        rgb_map, disp_map, acc_map, weights, depth_map = raw2outputs(raw, z_vals, rays_d, raw_noise_std, white_bkgd, pytest=pytest)\n",
    "\n",
    "    ret = {'rgb_map': rgb_map, 'disp_map': disp_map, 'acc_map': acc_map}\n",
    "    if retraw:\n",
    "        ret['raw'] = raw\n",
    "    if N_importance > 0:\n",
    "        ret['rgb0'] = rgb_map_0\n",
    "        ret['disp0'] = disp_map_0\n",
    "        ret['acc0'] = acc_map_0\n",
    "        ret['z_std'] = torch.std(z_samples, dim=-1, unbiased=False)\n",
    "\n",
    "    for k in ret:\n",
    "        if (torch.isnan(ret[k]).any() or torch.isinf(ret[k]).any()) and DEBUG:\n",
    "            print(f\"! [Numerical Error] {k} contains nan or int.\")\n",
    "\n",
    "    return ret\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def batchify_rays(rays_flat, chunk=1024*32, **kwargs):\n",
    "    all_ret = {}\n",
    "    for i in range(0, rays_flat.shape[0], chunk):\n",
    "        ret = render_rays(rays_flat[i:i+chunk], **kwargs)\n",
    "\n",
    "        for k in ret:\n",
    "            if k not in all_ret:\n",
    "                all_ret[k] = []\n",
    "            all_ret[k].append(ret[k])\n",
    "\n",
    "    all_ret = {k : torch.cat(all_ret[k], 0) for k in all_ret}\n",
    "    return all_ret"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def render(H, W, K, chunk=1024*32, rays=None, c2w=None, ndc=True, near=0, far=1, use_viewdirs=False, c2w_staticcam=None, **kwargs):\n",
    "    if c2w is not None:\n",
    "        rays_o, rays_d = get_rays(H,W,K,c2w)\n",
    "    else:\n",
    "        rays_o, rays_d = rays\n",
    "\n",
    "    if use_viewdirs:\n",
    "        viewdirs = rays_d\n",
    "        if c2w_staticcam is not None: #relighting 등\n",
    "            rays_o, rays_d = get_rays(H,W,K,c2w_staticcam)\n",
    "        viewdirs = viewdirs / torch.norm(viewdirs, dim=-1, keepdim=True)\n",
    "        viewdirs = torch.reshape(viewdirs, [-1,3]).float()\n",
    "\n",
    "    sh = rays_d.shape\n",
    "    # if ndc: #ray가 표현하는 공간은 육면체형태인데 이를 직육면체 형태로 normalize하는 방식 -> 360Scene에 대해서는 어려움.\n",
    "    #     rays_o, rays_d = ndc_rays(H,W,K[0][0], 1., rays_o, rays_d)\n",
    "\n",
    "    rays_o = torch.reshape(rays_o, [-1,3]).float() #flatten\n",
    "    rays_d = torch.reshape(rays_d, [-1,3]).float()\n",
    "\n",
    "    near, far = near * torch.ones_like(rays_d[...,:1]), far * torch.ones_like(rays_d[...,:1])\n",
    "    rays = torch.cat([rays_o, rays_d, near, far], -1) # (B, 8)\n",
    "    if use_viewdirs:\n",
    "        rays = torch.cat([rays, viewdirs], -1) # (B, 11) #ray batch화\n",
    "\n",
    "    # Render and reshape\n",
    "    all_ret = batchify_rays(rays, chunk, **kwargs)\n",
    "    for k in all_ret:\n",
    "        k_sh = list(sh[:-1]) + list(all_ret[k].shape[1:]) #다시 원래 dimension 복구\n",
    "        all_ret[k] = torch.reshape(all_ret[k], k_sh)\n",
    "\n",
    "    k_extract = ['rgb_map', 'disp_map', 'acc_map']\n",
    "    ret_list = [all_ret[k] for k in k_extract]\n",
    "    ret_dict = {k : all_ret[k] for k in all_ret if k not in k_extract}\n",
    "    return ret_list + [ret_dict]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def render_path(render_poses, hwf, K, chunk, render_kwargs, gt_imgs=None, savedir=None, render_factor=0):\n",
    "\n",
    "    H, W, focal = hwf\n",
    "\n",
    "    if render_factor!=0:\n",
    "        # Render downsampled for speed\n",
    "        H = H//render_factor\n",
    "        W = W//render_factor\n",
    "        focal = focal/render_factor\n",
    "\n",
    "    rgbs = []\n",
    "    disps = []\n",
    "\n",
    "    t = time.time()\n",
    "    for i, c2w in enumerate(tqdm(render_poses)):\n",
    "        print(i, time.time() - t)\n",
    "        t = time.time()\n",
    "        rgb, disp, acc, _ = render(H, W, K, chunk=chunk, c2w=c2w[:3,:4], **render_kwargs)\n",
    "        rgbs.append(rgb.cpu().numpy())\n",
    "        disps.append(disp.cpu().numpy())\n",
    "        if i==0:\n",
    "            print(rgb.shape, disp.shape)\n",
    "\n",
    "        \"\"\"\n",
    "        if gt_imgs is not None and render_factor==0:\n",
    "            p = -10. * np.log10(np.mean(np.square(rgb.cpu().numpy() - gt_imgs[i])))\n",
    "            print(p)\n",
    "        \"\"\"\n",
    "\n",
    "        if savedir is not None:\n",
    "            rgb8 = to8b(rgbs[-1])\n",
    "            filename = os.path.join(savedir, '{:03d}.png'.format(i))\n",
    "            imageio.imwrite(filename, rgb8)\n",
    "\n",
    "\n",
    "    rgbs = np.stack(rgbs, 0)\n",
    "    disps = np.stack(disps, 0)\n",
    "\n",
    "    return rgbs, disps"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "img2mse = lambda x, y : torch.mean((x-y) ** 2)\n",
    "mse2psnr = lambda x : -10. * torch.log(x) / torch.log(torch.Tensor([10.]))\n",
    "to8b = lambda x : (255*np.clip(x,0,1)).astype(np.uint8)\n",
    "\n",
    "def train(args):\n",
    "\n",
    "    # Load data\n",
    "    K = None\n",
    "    # if args.dataset_type == 'llff':\n",
    "    #     images, poses, bds, render_poses, i_test = load_llff_data(args.datadir, args.factor,\n",
    "    #                                                               recenter=True, bd_factor=.75,\n",
    "    #                                                               spherify=args.spherify)\n",
    "    #     hwf = poses[0,:3,-1]\n",
    "    #     poses = poses[:,:3,:4]\n",
    "    #     print('Loaded llff', images.shape, render_poses.shape, hwf, args.datadir)\n",
    "    #     if not isinstance(i_test, list):\n",
    "    #         i_test = [i_test]\n",
    "    #\n",
    "    #     if args.llffhold > 0:\n",
    "    #         print('Auto LLFF holdout,', args.llffhold)\n",
    "    #         i_test = np.arange(images.shape[0])[::args.llffhold]\n",
    "    #\n",
    "    #     i_val = i_test\n",
    "    #     i_train = np.array([i for i in np.arange(int(images.shape[0])) if\n",
    "    #                     (i not in i_test and i not in i_val)])\n",
    "    #\n",
    "    #     print('DEFINING BOUNDS')\n",
    "    #     if args.no_ndc:\n",
    "    #         near = np.ndarray.min(bds) * .9\n",
    "    #         far = np.ndarray.max(bds) * 1.\n",
    "    #\n",
    "    #     else:\n",
    "    #         near = 0.\n",
    "    #         far = 1.\n",
    "    #     print('NEAR FAR', near, far)\n",
    "\n",
    "    if args.dataset_type == 'blender':\n",
    "        images, poses, render_poses, hwf, i_split = load_blender_data(args.datadir, args.half_res, args.testskip)\n",
    "        print('Loaded blender', images.shape, render_poses.shape, hwf, args.datadir)\n",
    "        i_train, i_val, i_test = i_split\n",
    "\n",
    "        near = 2.\n",
    "        far = 6.\n",
    "\n",
    "        if args.white_bkgd:\n",
    "            images = images[...,:3]*images[...,-1:] + (1. - images[...,-1:])\n",
    "        else:\n",
    "            images = images[...,:3]\n",
    "\n",
    "    else:\n",
    "        print('Unknown dataset type', args.dataset_type, 'exiting')\n",
    "        return\n",
    "\n",
    "    # Cast intrinsics to right types\n",
    "    H, W, focal = hwf\n",
    "    H, W = int(H), int(W)\n",
    "    hwf = [H, W, focal]\n",
    "\n",
    "    if K is None:\n",
    "        K = np.array([\n",
    "            [focal, 0, 0.5*W],\n",
    "            [0, focal, 0.5*H],\n",
    "            [0, 0, 1]\n",
    "        ])\n",
    "\n",
    "    if args.render_test:\n",
    "        render_poses = np.array(poses[i_test])\n",
    "\n",
    "    # Create log dir and copy the config file\n",
    "    basedir = args.basedir\n",
    "    expname = args.expname\n",
    "    os.makedirs(os.path.join(basedir, expname), exist_ok=True)\n",
    "    f = os.path.join(basedir, expname, 'args.txt')\n",
    "    with open(f, 'w') as file:\n",
    "        for arg in sorted(vars(args)):\n",
    "            attr = getattr(args, arg)\n",
    "            file.write('{} = {}\\n'.format(arg, attr))\n",
    "    if args.config is not None:\n",
    "        f = os.path.join(basedir, expname, 'config.txt')\n",
    "        with open(f, 'w') as file:\n",
    "            file.write(open(args.config, 'r').read())\n",
    "\n",
    "    # Create nerf model\n",
    "    render_kwargs_train, render_kwargs_test, start, grad_vars, optimizer = create_nerf(args)\n",
    "    global_step = start\n",
    "\n",
    "    bds_dict = {\n",
    "        'near' : near,\n",
    "        'far' : far,\n",
    "    }\n",
    "    render_kwargs_train.update(bds_dict)\n",
    "    render_kwargs_test.update(bds_dict)\n",
    "\n",
    "    # Move testing data to GPU\n",
    "    render_poses = torch.Tensor(render_poses).to(device)\n",
    "\n",
    "    # Short circuit if only rendering out from trained model\n",
    "    if args.render_only:\n",
    "        print('RENDER ONLY')\n",
    "        with torch.no_grad():\n",
    "            if args.render_test:\n",
    "                # render_test switches to test poses\n",
    "                images = images[i_test]\n",
    "            else:\n",
    "                # Default is smoother render_poses path\n",
    "                images = None\n",
    "\n",
    "            testsavedir = os.path.join(basedir, expname, 'renderonly_{}_{:06d}'.format('test' if args.render_test else 'path', start))\n",
    "            os.makedirs(testsavedir, exist_ok=True)\n",
    "            print('test poses shape', render_poses.shape)\n",
    "\n",
    "            rgbs, _ = render_path(render_poses, hwf, K, args.chunk, render_kwargs_test, gt_imgs=images, savedir=testsavedir, render_factor=args.render_factor)\n",
    "            print('Done rendering', testsavedir)\n",
    "            imageio.mimwrite(os.path.join(testsavedir, 'video.mp4'), to8b(rgbs), fps=30, quality=8)\n",
    "\n",
    "            return\n",
    "\n",
    "    # Prepare raybatch tensor if batching random rays\n",
    "    N_rand = args.N_rand\n",
    "    use_batching = not args.no_batching\n",
    "    if use_batching:\n",
    "        # For random ray batching\n",
    "        print('get rays')\n",
    "        rays = np.stack([get_rays_np(H, W, K, p) for p in poses[:,:3,:4]], 0) # [N, ro+rd, H, W, 3]\n",
    "        print('done, concats')\n",
    "        rays_rgb = np.concatenate([rays, images[:,None]], 1) # [N, ro+rd+rgb, H, W, 3]\n",
    "        rays_rgb = np.transpose(rays_rgb, [0,2,3,1,4]) # [N, H, W, ro+rd+rgb, 3]\n",
    "        rays_rgb = np.stack([rays_rgb[i] for i in i_train], 0) # train images only\n",
    "        rays_rgb = np.reshape(rays_rgb, [-1,3,3]) # [N'*H*W, ro+rd+rgb, 3]\n",
    "        rays_rgb = rays_rgb.astype(np.float32)\n",
    "        print('shuffle rays')\n",
    "        np.random.shuffle(rays_rgb)\n",
    "\n",
    "        print('done')\n",
    "        i_batch = 0\n",
    "\n",
    "    # Move training data to GPU\n",
    "    if use_batching:\n",
    "        images = torch.Tensor(images).to(device)\n",
    "    poses = torch.Tensor(poses).to(device)\n",
    "    if use_batching:\n",
    "        rays_rgb = torch.Tensor(rays_rgb).to(device)\n",
    "\n",
    "\n",
    "    N_iters = 200000 + 1\n",
    "    print('Begin')\n",
    "    print('TRAIN views are', i_train)\n",
    "    print('TEST views are', i_test)\n",
    "    print('VAL views are', i_val)\n",
    "\n",
    "    # Summary writers\n",
    "    # writer = SummaryWriter(os.path.join(basedir, 'summaries', expname))\n",
    "    psnrs = []\n",
    "    iternums = []\n",
    "    start = start + 1\n",
    "    for i in trange(start, N_iters):\n",
    "        time0 = time.time()\n",
    "\n",
    "        # Sample random ray batch\n",
    "        if use_batching:\n",
    "            # Random over all images\n",
    "            batch = rays_rgb[i_batch:i_batch+N_rand] # [B, 2+1, 3*?]\n",
    "            batch = torch.transpose(batch, 0, 1)\n",
    "            batch_rays, target_s = batch[:2], batch[2]\n",
    "\n",
    "            i_batch += N_rand\n",
    "            if i_batch >= rays_rgb.shape[0]:\n",
    "                print(\"Shuffle data after an epoch!\")\n",
    "                rand_idx = torch.randperm(rays_rgb.shape[0])\n",
    "                rays_rgb = rays_rgb[rand_idx]\n",
    "                i_batch = 0\n",
    "\n",
    "        else:\n",
    "            # Random from one image\n",
    "            img_i = np.random.choice(i_train)\n",
    "            target = images[img_i]\n",
    "            target = torch.Tensor(target).to(device)\n",
    "            pose = poses[img_i, :3,:4]\n",
    "\n",
    "            if N_rand is not None:\n",
    "\n",
    "                rays_o, rays_d = get_rays(H, W, K, torch.Tensor(pose))  # (H, W, 3), (H, W, 3)\n",
    "\n",
    "                if i < args.precrop_iters:\n",
    "                    dH = int(H//2 * args.precrop_frac)\n",
    "                    dW = int(W//2 * args.precrop_frac)\n",
    "                    coords = torch.stack(\n",
    "                        torch.meshgrid(\n",
    "                            torch.linspace(H//2 - dH, H//2 + dH - 1, 2*dH),\n",
    "                            torch.linspace(W//2 - dW, W//2 + dW - 1, 2*dW)\n",
    "                        ), -1)\n",
    "                    if i == start:\n",
    "                        print(f\"[Config] Center cropping of size {2*dH} x {2*dW} is enabled until iter {args.precrop_iters}\")\n",
    "                else:\n",
    "                    coords = torch.stack(torch.meshgrid(torch.linspace(0, H-1, H), torch.linspace(0, W-1, W)), -1)  # (H, W, 2)\n",
    "\n",
    "                coords = torch.reshape(coords, [-1,2])  # (H * W, 2)\n",
    "                select_inds = np.random.choice(coords.shape[0], size=[N_rand], replace=False)  # (N_rand,)\n",
    "                select_coords = coords[select_inds].long()  # (N_rand, 2)\n",
    "                rays_o = rays_o[select_coords[:, 0], select_coords[:, 1]]  # (N_rand, 3)\n",
    "                rays_d = rays_d[select_coords[:, 0], select_coords[:, 1]]  # (N_rand, 3)\n",
    "                batch_rays = torch.stack([rays_o, rays_d], 0) # (2, N_rand, 3)\n",
    "                target_s = target[select_coords[:, 0], select_coords[:, 1]]  # (N_rand, 3)\n",
    "\n",
    "        #####  Core optimization loop  #####\n",
    "        rgb, disp, acc, extras = render(H, W, K, chunk=args.chunk, rays=batch_rays,verbose=i < 10, retraw=True,**render_kwargs_train)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        img_loss = img2mse(rgb, target_s)\n",
    "        loss = img_loss\n",
    "        psnr = mse2psnr(img_loss)\n",
    "\n",
    "        if 'rgb0' in extras:\n",
    "            img_loss0 = img2mse(extras['rgb0'], target_s)\n",
    "            loss = loss + img_loss0\n",
    "            psnr0 = mse2psnr(img_loss0)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # NOTE: IMPORTANT!\n",
    "        ###   update learning rate   ###\n",
    "        decay_rate = 0.1\n",
    "        decay_steps = args.lrate_decay * 1000\n",
    "        new_lrate = args.lrate * (decay_rate ** (global_step / decay_steps))\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = new_lrate\n",
    "        ################################\n",
    "\n",
    "        dt = time.time()-time0\n",
    "        # print(f\"Step: {global_step}, Loss: {loss}, Time: {dt}\")\n",
    "        #####           end            #####\n",
    "\n",
    "        # Rest is logging\n",
    "        if i%args.i_weights==0:\n",
    "            path = os.path.join(basedir, expname, '{:06d}.tar'.format(i))\n",
    "            torch.save({\n",
    "                'global_step': global_step,\n",
    "                'network_fn_state_dict': render_kwargs_train['network_fn'].state_dict(),\n",
    "                'network_fine_state_dict': render_kwargs_train['network_fine'].state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, path)\n",
    "            print('Saved checkpoints at', path)\n",
    "\n",
    "        if i%args.i_video==0 and i > 0:\n",
    "            # Turn on testing mode\n",
    "            with torch.no_grad():\n",
    "                rgbs, disps = render_path(render_poses, hwf, K, args.chunk, render_kwargs_test)\n",
    "            print('Done, saving', rgbs.shape, disps.shape)\n",
    "            moviebase = os.path.join(basedir, expname, '{}_spiral_{:06d}_'.format(expname, i))\n",
    "            imageio.mimwrite(moviebase + 'rgb.mp4', to8b(rgbs), fps=30, quality=8)\n",
    "            imageio.mimwrite(moviebase + 'disp.mp4', to8b(disps / np.max(disps)), fps=30, quality=8)\n",
    "\n",
    "\n",
    "        if i%args.i_testset==0 and i > 0:\n",
    "            testsavedir = os.path.join(basedir, expname, 'testset_{:06d}'.format(i))\n",
    "            os.makedirs(testsavedir, exist_ok=True)\n",
    "            print('test poses shape', poses[i_test].shape)\n",
    "            with torch.no_grad():\n",
    "                render_path(torch.Tensor(poses[i_test]).to(device), hwf, K, args.chunk, render_kwargs_test, gt_imgs=images[i_test], savedir=testsavedir)\n",
    "            print('Saved test set')\n",
    "\n",
    "\n",
    "\n",
    "        if i%args.i_print==0:\n",
    "            tqdm.write(f\"[TRAIN] Iter: {i} Loss: {loss.item()}  PSNR: {psnr.item()}\")\n",
    "\n",
    "            testpose = poses[18]\n",
    "            # Render the holdout view for logging\n",
    "            with torch.no_grad():\n",
    "                rgb, depth, acc, _ = render(H, W, K, c2w=testpose[:3,:4], **render_kwargs_test)\n",
    "\n",
    "            psnrs.append(psnr.detach().cpu().numpy())\n",
    "            iternums.append(i)\n",
    "\n",
    "            plt.figure(figsize=(10,4))\n",
    "            plt.subplot(121)\n",
    "            plt.imshow(rgb.cpu())\n",
    "            plt.title(f'Iteration: {i}')\n",
    "            plt.subplot(122)\n",
    "            plt.plot(iternums, psnrs)\n",
    "            plt.title('PSNR')\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "\n",
    "        global_step += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "train(args)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}